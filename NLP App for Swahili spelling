import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import pandas as pd
data = pd.read_csv("media_data.csv")

import SwahiliStemmer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')

# Load stop words
swahili_stopwords = set(stopwords.words('swahili'))

def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    
    # Remove special characters and punctuations
    text = re.sub('[^a-zA-Z0-9]', ' ', text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Tokenize text
    tokens = text.split()
    
    # Remove stop words
    tokens = [token for token in tokens if token not in swahili_stopwords]
    
    # Stem words
    stemmer = SwahiliStemmer.Stemmer()
    tokens = [stemmer.stemWord(token) for token in tokens]
    
    # Join tokens back into a sentence
    clean_text = ' '.join(tokens)
    
    return clean_text
    
    import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Load data
data = pd.read_csv('data.csv')

# Clean text data
data['clean_text'] = data['text'].apply(lambda x: clean_text(x))

# Create feature vectors
vectorizer = CountVectorizer()
features = vectorizer.fit_transform(data['clean_text'])

# Print feature vectors
print(features.toarray())
